---
id: portfolio-risk-mitigation
title: Portfolio Risk Mitigation
sidebar_position: 6
---

# And an Emphasis on Portfolio Risk Mitigation

The EthicalOS Risk Mitigation Checklist was developed by the Institute for the Future (IFTF) as part of the broader Ethical OS Toolkit. The toolkit was created to help organizations anticipate and navigate the ethical implications of technology. While not as widely known as some other frameworks, it has been utilized by organizations committed to responsible innovation.

We will review this list of questions with you, and determine which of these are applicable to your particular product and business organization. We will then discuss and answer each relevant question to ensure we are addressing any potential risks associated with our data product. 

1. **Risk Zone 1: Truth, Disinformation, Propaganda**
   - What type of data do users expect you to accurately share, measure, or collect?
   - How could bad actors use your tech to subvert or attack the truth? What could become the equivalent of fake news, bots, or deep fakes, on your platform?
   - How could someone use this technology to undermine trust in established social institutions, like media, medicine, democracy, science? Could your tech be used to generate or spread misinformation to create political distrust or social unrest?
   - Imagine the form such misinformation might take on your platform.
   - Even if your tech is meant to be apolitical in nature, how could it be co-opted to destabilize a government?

2. **Risk Zone 2: Addiction & the Dopamine Economy**
   - Does the business model behind your chosen technology benefit from maximizing user attention and engagement—i.e., the more, the better? If so, is that good for the mental, physical, or social health of the people who use it? What might not be good about it?
   - What does “extreme” use of, addiction to, or unhealthy engagement with your tech look like?
   - What does “moderate” use to or healthy engagement look like?
   - How could you design a system that encourages moderate use? Can you imagine a business model where promoting moderate use is more sustainable or profitable than always seeking to increase or maximize engagement?
   - If there is potential for toxic materials like conspiracy theories and propaganda to drive high levels of engagement, what steps are being taken to reduce the prevalence of that content? Is it enough?

3. **Risk Zone 3: Economic & Asset Inequalities**
   - Who will have access to this technology and who won’t? Will people or communities who don’t have access to this technology suffer a setback compared to those who do?
   - What does that setback look like? What new differences will there be between the “haves” and “have-nots” of this technology?
   - What asset does your technology create, collect, or disseminate? (example: health data, gigs, a virtual currency, deep AI) Who has access to this asset? Who has the ability to monetize it?
   - Is the asset (or profits from it) fairly shared or distributed with other parties who help create or collect it?
   - Are you using machine learning and robots to create wealth, rather than human labor? If you are reducing human employment, how might that impact overall economic well-being and social stability? Are there other ways your company or product can contribute to our collective economic security, if not through employment of people?

4. **Risk Zone 4: Machine Ethics & Algorithmic Biases**
   - Does this technology make use of deep data sets and machine learning? If so, are there gaps or historical biases in the data that might bias the technology?
   - Have you seen instances of personal or individual bias enter into your product’s algorithms?
   - How could these have been prevented or mitigated?
   - Is the technology reinforcing or amplifying existing bias?
   - Who is responsible for developing the algorithm? Is there a lack of diversity in the people responsible for the design of the technology?
   - How will you push back against a blind preference for automation (the assumption that AI-based systems and decisions are correct and don’t need to be verified or audited)?
   - Are your algorithms transparent to the people impacted by them? Is there any recourse for people who feel they have been incorrectly or unfairly assessed?

5. **Risk Zone 5: Surveillance State**
   - How might a government or military body utilize this technology to increase its capacity to surveil or otherwise infringe upon the rights of its citizens?
   - What could governments do with this data if they were granted access to it, or if they legally required or subpoenaed access to it?
   - Who, besides government or military, might use the tools and data you’re creating to increase surveillance of targeted individuals? Whom would they track, why—and do you want your tech to be used in this way?
   - Are you creating data that could follow users throughout their lifetimes, affect their reputations, and impact their future opportunities? Will the data your tech is generating have long-term consequences for the freedoms and reputation of individuals?
   - Whom would you not want to use your data to surveil and make decisions about individuals, and why not? What can you do to proactively protect this data from being accessible to them?

6. **Risk Zone 6: Data Control & Monetization**
   - What data are you collecting from users? Do you really need to collect it? Are you selling it?
   - If so, whom are you selling it to and do users know this? How can you be more transparent about this?
   - Do your users have the right and ability to access the data you have collected about them? If not, how can you better support users in easily and transparently knowing about themselves what you already know about them?
   - If you profit from the use or sale of user data, do your users share in that profit? What options would you consider for giving users the right to share profits on their own data?
   - Could you build ways to give users the right to share and monetize their own independently?
   - What could bad actors do with this data if they had access to it? What is the worst thing someone could do with this data if it were stolen or leaked?
   - Do you have a policy in place for what happens to customer data if your company is bought, sold, or shut down?

7. **Risk Zone 7: Implicit Trust & User Understanding**
   - Does the technology you’re building have a clear code of rights for users? Are the terms of service easy to read, access, and understand?
   - Is there a version of your product that is available to use if users don’t want to sign the user agreements?
   - Does your technology do anything your users don’t know about, or would probably be surprised to find out about? If so, why are you not sharing this information explicitly—and what kind of backlash might you face if users found out?
   - If users object to the idea of their actions being monetized, or their data being sold to specific types of groups or organizations, though still want to use the platform, what options do they have? Is it possible to create alternative models that builds trust and allows users to opt-in or opt-out of different aspects of your business model moving forward?
   - Are all users treated equally? If not—and your algorithms and predictive technologies prioritize certain information or set prices or access differently for different users—how would you handle consumer demands or government regulations that require all users be treated equally, or at least transparently unequally?

8. **Risk Zone 8: Hateful & Criminal Actors**
   - How could someone use your technology to bully, stalk, or harass other people?
   - What new kinds of ransomware, theft, financial crimes, fraud, or other illegal activity could potentially arise in or around your tech?
   - Do technology makers have an ethical responsibility to make it harder for bad actors to act?
   - How could organized hate groups use your technology to spread hate, recruit, or discriminate against others? What does organized hate look like on your platform or community or users?
   - What are the risks of your technology being weaponized? What responsibility do you have to prevent this? How do you work to create regulations or international treaties to prevent the weaponizing of technology?

**General Questions for Product Managers**
   - Have you assessed not only the first-order consequences of your product, service or platform, but also its second- and third-order impacts?
   - Have you designed your product or service in such a way so as to ensure that a diverse set of users will be able to access?
   - Have you ensured you’re collecting only as much user data as is absolutely necessary to operate your product and ensure your financial viability?
   - Have you made certain that your training data or data sets represent a diverse set of users and that you’ve minimized any potential bias in the source of that data?
   - Have you identified the most effective ways to ensure your product spreads only truthful content and information?
   - Have you ensured the fairness, accountability, and transparency of any algorithms or machine learning processes embedded in your product or service?
   - Have you designed appropriate off-ramps for your users? Have you eliminated, to the extent possible, bottomless pits, infinite scrolls, and attention traps?
   - Have you found ways to avoid the use of any dark patterns in your UX?
   - If you have them, are your notifications important enough to interrupt the user? Can they be delivered in a less intrusive manner—using, for example, a sound, a sensation, or subtle status change?
   - Have you created systems that place a value on quality of user-generated content over quantity?
   - Have you created contingency plans for data breach?
   - Have you found ways to ensure that users will trust you before, during, and after such a breach?

**Questions To Ask Before Shipping a Product**
    - Have you written your terms of service clearly, succinctly, and in easy-to-understand language?
    - Have you tested your product with a diverse set of users, representing diversity of age, gender, race, socioeconomic status and income, geography, political affiliation, language, ability, sexual orientation, religion, and education?
    - Have you re-examined any potentially discarded paths to revenue or growth that you should reconsider in light of any of the unintended consequences you can foresee?
    - Have you red-teamed your product to assess how a bad or malevolent actor (individual, group, or body) might weaponize your product?
